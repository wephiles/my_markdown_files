- \### 爬虫学习

  - \- 爬虫在法律中是不被禁止

  - \- 具有违法风险

  - \- 时常优化自己的程序 避免干扰被访问网站的运行

  - \- 避免爬取涉及到个人隐私的时候，需要小心

- 一、 爬虫的分类

  - ​    1. 通用爬虫：

    - 1.1 抓取系统重要组成部分

    - 1.2 抓取的是互联网中的一整张的页面

  - ​    2. 聚焦爬虫

    -  2.1 建立在通用爬虫的基础之上

    -  2.2 抓取的是页面中特定的局部内容

  - 3. 增量式爬虫

    -  3.1 监测网站中数据更新的情况

    -  3.2 只会抓取网站中最行新更新的程序

- 二、 爬虫的矛与盾

- 三、 反爬机制

  - ​    1. 门户网站可以通过制定相关的策略或者技术手段，防止爬虫程序爬虫进行网站数据的爬取

    

- 四、 反反爬策略

  - ​    1. 爬虫程序可以通过制定相关的策略或者技术手段，破解门户网站的反爬策略，从而获取门户网站的信息

    

- 五、 robots.txt 协议

  - ​    君子协议，规定网站具有规定哪些网站信息可以被爬虫，哪些数据不可以被爬虫。

  - ​    查看网站哪些内容是可以被爬取的，哪些不可以被爬起的方法：在浏览器域名后面输入 /robots.txt

- 六、 http协议

  - ​    1. 概念：服务器和客户端进行数据交互的一种形式

    

- 七、 常用请求头信息

  - ​    1. User-Agent:表示请求载体的身份标识![img](https://api2.mubu.com/v3/document_image/697abce5-4e35-4a9f-958c-8ad1bba28261-15396883.jpg)

    

  - Connection: 请求完毕之后 是断开连接还是保持连接![img](https://api2.mubu.com/v3/document_image/d5177bd8-cae0-415f-afd1-f1e15a866cfe-15396883.jpg)

- 八、 https 协议

  - 概念：安全地 http 协议 超文本传输协议 数据加密

- 九、常用响应头信息

  - Content-Type: 服务器响应回客户端的数据类型![img](https://api2.mubu.com/v3/document_image/39055b12-c562-4416-9219-44df9d31e8de-15396883.jpg)

- 十、 加密方式

  - 对称密钥加密：客户端向服务端发请求 传递的数据采用堆成的密钥加密 共享密钥加密 安全性不太好

  - 非对称密钥加密：私有、公有密钥 效率低 存在安全隐患 公钥容易被篡改

  - 证书加密方式（https）：证书认证机构 是一个中间机构   客户端和服务器足够信任

- 十一、requests 模块

  - urllib模块

  - requests模块：

    - 概念：python中原生的基于网络请求的模块 功能强大 简单便捷 效率极高

    - 作用：模拟浏览器发请求

    - 使用：requests模块的编码流程

      - 指定url

      - 对url发起一个请求

      - 获取响应数据

      - 持久化存储

    - 环境安装：pip install requests

    - 实战编码

      - 需求：爬取搜狗首页的页面数据![img](https://api2.mubu.com/v3/document_image/62ad246d-fa96-4f99-8174-d94d861ccf90-15396883.jpg)![img](https://api2.mubu.com/v3/document_image/8c9d486a-3c0b-47e5-b78a-20391e1cf197-15396883.jpg)

      - python爬取的页面全在一行，那么就要导入bs4模块中的BeautifulSoup，首先BeautifulSoup(爬取到的网页文件，'lxml') 注意需要导入lxml库    再使用 .prettify()函数即可
      - ![img](https://api2.mubu.com/v3/document_image/60ebdc71-25f7-432a-8e5f-9672bc4eed79-15396883.jpg)

    - 实战演练 进阶

      - 需求：爬起搜狗指定词条相应的搜索结果页面（简易网页采集器）

      - 需求：破解百度翻译

        - post请求，携带了参数

        - 响应数据是一组json数据

      - 需求：爬取豆瓣电影分类排行榜 https://movie.douban.com/中的电影数据详情![img](https://api2.mubu.com/v3/document_image/ee9b25fe-40f1-45b1-b084-387e12badc38-15396883.jpg)

      - 作业：爬取肯德基餐厅查询http://www.kfc.com.cn/kfccda/index.aspx中的餐厅数![img](https://api2.mubu.com/v3/document_image/dccae967-c8cd-4809-93a3-f5ac0227806a-15396883.jpg)

      - 需求：爬取国家药品监管总局中基于中华人民共和国化妆品生产许可证相关数据http://125.35.6.84:81/xk/       http://scxk.nmpa.gov.cn:81/xk/

        通过网页抓包工具测试所获得的数据是否是由网页刷新所得到的。

        - 动态加载数据

        - 首页中对应的企业信息是通过Ajax动态请求到的

        - 获取href超链接：http://scxk.nmpa.gov.cn:81/xk/itownet/portal/dzpz.jsp?id=af4832c505b749dea76e22a193f873c6

        - 通过对详情页url的观察，发现其ID参数不一样，其他的都是一样的，而ID 可以从AJAX请求获得的JSON传串中获得，域名和ID值拼接到一起，成为一个完整的企业对应的详情页url

        - 详情页的企业详情信息也是动态加载出来的

        - 观察后发现所有post的请求的url的请求都是一样的只有参数ID值是不同的，如果我们可以批量获取多加企业的id ，则可以......

      - 补充：UA伪装（一种反爬机制）即User-Agent（请求载体的身份标识）让爬虫对应的请求载体身份标识伪装成一个浏览器。将对应的User-Agent封装到字典中
    UA 伪装指的是门户网站的服务器监测对应请求的载体身份标识，如果检测到为某一款浏览器，意味着是一个正常的用户发起一个正常的请求，但是如果监测到如果不是基于某一款浏览器，那么是基于不正常的请求——爬虫，服务器端就很有可能拒接该次请求。
  
- 十二、 数据解析（聚焦爬虫）——75%以上的数据都是用过聚焦爬虫爬取的

  - 聚焦爬虫：爬取页面指定的页面内容，建立在通用爬虫的基础之上；

    - 编码流程：

      - 1. 指定url

        

      - 2. 发起请求

        

      - 3. 获取响应数据

        

      - 4. 数据解析

        

      - 5. 持久化存储

        

  - 数据解析分类：

    - 正则表达式：

    - bs4模块中的BeautifulSoup：

    - xpath（重点）在其他语言也可以使用xpath，高性能

  - 数据解析原理概述：

    - 解析的局部文本内容都会在标签之间或者标签对应的属性中进行存储

    - 只需要定位到指定的标签就可

    - 1. 指定标签的定位

      

    - 2. 将标签或者标签对应属性中存储的数据值进行提取（解析）

      

  - 1. 正则表达式：可以应用在其他语言中去

    - 需求：爬取糗事百科中糗图的图片信息

    - text返回字符串形式的网页内容

    - content返回二进制的网页返回数据

    - JSON返回对象类型的返回数据

  - python源代码的阅读

- 十三、 bs4数据解析：

  - 1. python独有的

    

  - 2. bs4进行数据解析的方式：

    

  - 所有解析方式：

    - （1）标签定位

    - （2）提取标签

  - bs4数据解析的原理：

    - 1 标签定位 实例化一个BeautifulSoup对象 并且将页面源码数据加载到该对象中

    - 2 调用BeautifulSoup里面的相关属性或者方法进行定位以及数据提取

  - 环境安装 

    - pip install bs4

    - pip install lxml 解析器  xpath也会用到

  - 属性和方法：

    - \- 如何实例化beautifulsoup对象 ：

      - from bs4 import BeautifulSoup

      - 对象的实例化 

        - -将本地的HTML文档中的数据加载到该对象中![img](https://api2.mubu.com/v3/document_image/ef3467ef-b83e-4da8-8ccd-3437c4d4e037-15396883.jpg)

        - 将互联网中获取的页面源码加载到该对象中去

          - page_text = response.text

          - soup = BeautifulSoup(page_text, 'lxml')

    - 提供的用于数据解析的方法和属性：![img](https://api2.mubu.com/v3/document_image/ddb78024-61fc-4b53-965a-c730e4b666c3-15396883.jpg)

    - 1. soup.tagName：返回文档中第一次出来出现的tagName(标签名)的内容![img](https://api2.mubu.com/v3/document_image/00e51b89-870f-4485-b31e-e9cc2c987c8e-15396883.jpg)

      

    - 2. soup.find('tagName')：等同于soup.tagName

      - 属性定位：![img](https://api2.mubu.com/v3/document_image/c0ce1e1d-8f20-41cd-bb62-b79477043e86-15396883.jpg)

    - 3. soup.find_all('tagName'):返回一个列表（符合标准的所有标签）![img](https://api2.mubu.com/v3/document_image/348e2be7-cc58-4e8a-8d7f-10ceeac23b58-15396883.jpg)

      

    - 4. select：返回一个列表，也可以应用到层级选择器中print*(soup.select('#FileItems > ul > li')[1])*  大于号>表示的是一个层级   空格表示的是多个层级  print*(soup.select('#FileItems li')[0])*![img](https://api2.mubu.com/v3/document_image/064a87eb-528d-4479-8739-9f0e8e886d6b-15396883.jpg)![img](https://api2.mubu.com/v3/document_image/459694e6-969c-4403-a084-88dc68a714d8-15396883.jpg)![img](https://api2.mubu.com/v3/document_image/e411157c-2229-4f8f-9355-79d2ea085fc5-15396883.jpg)

      

    - 5. 获取标签之间的文本数据以及属性值：

      - 1. 获取文本数据：   soup.a.text/string/get_text()

        - soup.text/get_text()方法能够获取所有的文本内容，即使这些标签的内容是该标签下面的所有内容，不只是直系的内容    而string能够获取该标签下面直系的内容

      - 2. 获取属性值：  soup.a['href']     print*(soup.select('link')[1][*'href'])

        

- 十四、 xpath解析： 最常用且最便捷高效的一种解析方式，通用性最强的

  - 1. xpath解析原理：

    - \- 实例化一个etree对象，需要将被解析的源码数据加载到该对象中去

    - \- 调用etree对象重任的xpath方法结合xpath表达式实现标签的定位和内容的捕获

  - 2. 环境的安装

    - pip install lxml

  - 3. 实例化一个etree对象  from lxml import etree

    - 1 将本地的HTML文档中的源码数据加载到etree对象中
      - etree.parse(filePath)

    - 2 可以将从互联网上获取的源码数据加载到该对象中去
      - etree.HTML('page_text')